\section{Randomized Algorithms}

\subsection{The Union Bound}
Given events $\mathscr{E}_1, \mathscr{E}_2, \ldots, \mathscr{E}_n$, we have
\[
	\text{Pr} \left[ \bigcup_{i=1}^{n} \mathscr{E}_i \right] \leq \sum_{i=1}^n \text{Pr}\left[ \mathscr{E}_i \right]
\]

We usually want the union on the left-hand side to represent a 	``bad event'' that we're trying to avoid, and want a bound on its probability in terms of constituent ``bad events'' on the right-hand side.

%%%%%%%

\subsection{Verifying AB=C}

Choose a random vector $\overline{r} = (r_1, r_2, \ldots, r_n) \in \{0, 1\}^n$. Then compute $B\overline{r}$ and then $A(B\overline{r})$; finally, compute $C\overline{r}$. If $A(B\overline{r}) \neq C\overline{r}$, then $AB \neq C$.\\

If $AB \neq C$ and if $\overline{r}$ is chosen uniformly at random from $\{0, 1\}^n$, then $\text{Pr}(AB\overline{r} = C\overline{r}) \leq \frac{1}{2}$.

Repeated trials increase the runtime to $\Theta(kn^2)$, and probability of error at most $2^{-k}$.\\

If it returns false, then it is always right, but if it returns true, then it does so with some probability of error.

\subsection{Law of Total Probability}
Let $E_1 ... E_n$ be mutually disjoint events in the sample space $\Omega$ and let $\bigcup_{i=1}^{n} E_{i} = \Omega$. Then

\[
	\text{Pr}(B) = \sum_{i=1}^n \text{Pr}(B \cap E_i) = \sum_{i=1}^n \text{Pr}(B | E_i) \text{Pr}(E_i)
\]

%%%%%%%

\subsection{Expected Value}

\[
	E[X] = \sum_{j = 0}^\infty j \cdot \text{Pr}[X = j]
\]

Suppose we have a coin which has a probability $p > 0$ of landing heads, and a probability $1 - p$ of landing tails. If we flip the coin until we get a heads, what's the expected number of flips we will perform?\\

For $j > 0$, we have $\text{Pr}[X = j] = p(1 - p)^{j-1}$; in order for the process to take exactly $j$ steps, the first $j - 1$ flips must come up tails, and the $j$th must come up heads. Thus, $E[X] = \sum_{j = 0}^\infty j \cdot \text{Pr}[X = j] = \frac{1}{p}$.

\subsection{Linearity of Expectation}
Given 2 random vars $X$ and $Y$ in the same probability space, $E [X + Y] = E[ X ] + E [ Y ] $\\
Expected Value of sum of two dice throws=\\
2*1/36 + 3*1/6 + .... + 7*1/36+\\
3*1/36 + 4*1/6 + .... + 8*1/36 +\\ 
........................\\
.........................\\
7*1/36 + 8*1/6 + .... + 12*1/36\\     
=  7
  
Expected Value of sum of 2 dice throws = 2*(Expected value of one dice throw)
= 2*(1/6 + 2/6 + .... 6/6)
= 2*7/2
= 7 

Expected value of sum for n dice throws is = n * 7/2 = 3.5 * n 

\subsection{Memoryless guessing}
You attempt to guess cards uniformly at random as they are revealed in a shuffled deck. You can expect to get 1 correct, independent of $n$ (use Linearity of Expectation).

\subsection{Memory guessing}
Now, if you remember which cards have been revealed (and randomly choose un-revealed cards), you can expect to correctly predict $H(n) = \Theta(\log n)$ cards (this is the harmonic series).

\subsection{Coupon collection}
Suppose a cereal brand offers $n$ types of coupons in boxes, at random. How many do you expect to buy before getting a coupon of each type?\\

Let $j$ be the number of coupons already collected. Then $E[ X_{j}] = n / (n - j)$, since $( n - j ) / n$ is the odds of getting a new coupon. Thus, $E[X] = nH(n) = \Theta(n \log n)$.

\subsection{Conditional Expectation}
Define the \emph{conditional expectation} of $X$, given $\mathscr{E}$, to be the expected value of $X$ computed only over the part of the sample space corresponding to $\mathscr{E}$. Then $E[ X | \mathscr{E} ] =  \sum_{j=0}^{\infty} j \cdot \text{Pr} [ X = j | \mathscr{E} ]$.

\subsection{Max 3-SAT}
There is a randomized algo with polynomial expected run time that is guaranteed to produce a truth assignment satisfying at least a $7/8$ fraction of all clauses. We would need $8k$ trials to get the satisfying assignment.

\subsection{Quicksort}

when pivot is selected uniformly at random the expected number of comparisons in quicksort is $O(n \log n)$.

%%%

\subsection{Hashing}
Let $n$ be the number of items stored in the map, and $m$ be the size of the map.

\subsubsection{Uniform}
A hashing chosen uniformly at random means that for all $x$ and all $i$, $\text{Pr}[h(x) = i] = \frac{1}{m}$.

\subsubsection{Universal}
For any two items in the universe, the probability of collision is as small as possible: $\text{Pr}[h(x) = h(y)] = \frac{1}{m}$ for all $x \neq y$.

\subsubsection{Near-universal}
The probability of collision is \emph{close} to ideal: $\text{Pr}[h(x) = h(y)] \leq \frac{2}{m}$ for all $x \neq y$.

\subsubsection{k-uniform}
The probability that each key maps to the corresponding hash value is $\text{Pr}[\bigwedge_{j=1}^{k} h(x_j) = i_j)] = \frac{1}{m^{k}}$ for all distinct $x_1,\ldots,x_k$ and all $i_1,\ldots,i_k$.

\subsubsection{Load Factor}
For a universal hash function, the probability of collision is $E[C_{x, y}] = \text{Pr}[C_{x,y} = 1] = 1$ if $x = y$ and $1/m$ otherwise. Thus, the expected number of collisions is $\frac{n}{m}$. This load factor is $\alpha = n/m$.\\

\subsubsection{Runtime}
Using a linked list, we have a bound for searching the hash table: $\Omega(1 + \alpha)$. Using a balanced binary tree, we have a bound of $O(1 + \log \alpha)$.\\

If we instead create a recursive hash table, we end up with a bound of $O(\log_m n)$.

\subsubsection{Open Addressing}
If we simply attempt to fill empty spaces with values, rather than use chaining to address collisions, we reduce our expected time to $O(1)$, unless the hash table is almost completely full.